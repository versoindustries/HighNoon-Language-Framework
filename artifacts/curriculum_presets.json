{
    "version": "2.0.0",
    "last_updated": "2026-01-03",
    "description": "Comprehensive curriculum presets for frontier-class LLM training",
    "presets": {
        "verso-baseline": {
            "name": "Verso Baseline (12-Stage)",
            "description": "The Ultimate 12-stage curriculum. Superset of all domains: 180+ datasets covering Deep Code, Quantum/Sci/Eng, Legal/Fin/Med, and Agentic capability.",
            "version": "4.0.0",
            "stages": [
                {
                    "name": "Stage 1: Foundation - General Pre-training",
                    "order": 1,
                    "epochs": 1,
                    "description": "Massive web-scale text ingestion (>50T tokens) for broad world knowledge",
                    "datasets": [
                        "HuggingFaceFW/fineweb",
                        "HuggingFaceFW/fineweb-edu",
                        "HuggingFaceFW/fineweb-2",
                        "allenai/dolma",
                        "pleias/common_corpus",
                        "cerebras/SlimPajama-627B",
                        "togethercomputer/RedPajama-Data-V2",
                        "eleutherai/pile",
                        "tiiuae/falcon-refinedweb",
                        "openwebtext",
                        "wikimedia/wikipedia",
                        "bookcorpus",
                        "HuggingFaceTB/cosmopedia",
                        "allenai/c4"
                    ]
                },
                {
                    "name": "Stage 2: Foundation - Multilingual",
                    "order": 2,
                    "epochs": 1,
                    "description": "Establishing cross-lingual bases (500+ languages) for global understanding",
                    "datasets": [
                        "cis-lmu/Glot500",
                        "facebook/flores",
                        "allenai/MADLAD-400",
                        "Helsinki-NLP/opus-100",
                        "CohereForAI/aya_collection",
                        "CohereForAI/aya_dataset",
                        "bigscience/xP3",
                        "facebook/xnli",
                        "paws-x",
                        "wiki_lingua"
                    ]
                },
                {
                    "name": "Stage 3: Code - Fundamentals (The Stack)",
                    "order": 3,
                    "epochs": 2,
                    "description": "Massive raw code ingestion. The Stack v1/v2 + DeepSeek Coder base.",
                    "datasets": [
                        "bigcode/the-stack-v2",
                        "bigcode/the-stack-v2-dedup",
                        "bigcode/the-stack",
                        "bigcode/the-stack-dedup",
                        "bigcode/starcoderdata",
                        "nvidia/Nemotron-Pretraining-Code-v2",
                        "deepseek-ai/DeepSeek-Coder-V2-Instruct",
                        "codeparrot/github-code",
                        "code-search-net/code_search_net"
                    ]
                },
                {
                    "name": "Stage 4: Code - Python Specialist",
                    "order": 4,
                    "epochs": 2,
                    "description": "Deep dive into Python ecosystem, libraries, educational code, and StackOverflow.",
                    "datasets": [
                        "cassanof/CodeLlama-Python-200k",
                        "flytech/python-codes-25k",
                        "iamtarun/python_code_instructions_18k_alpaca",
                        "jon-tow/starcoderdata-python-edu",
                        "mikex86/stackoverflow-posts"
                    ]
                },
                {
                    "name": "Stage 5: Code - Systems, Web & Engineering",
                    "order": 5,
                    "epochs": 2,
                    "description": "Systems (C++/Rust), Web (JS/TS/HTML), and core Engineering (Electrical, Civil, Robotics, CAD).",
                    "datasets": [
                        "STEM-AI-mtl/Electrical-engineering",
                        "lamm-mit/MechanicsMaterials",
                        "XXCCF/bridge_construction",
                        "LouisChen15/ConstructionSite",
                        "riegel/crackenpy_dataset",
                        "archanatikayatray/aeroBERT-NER",
                        "kyleeasterly/purple-aerospace-mix-v1-80-12",
                        "lerobot/aloha_mobile_cabinet",
                        "jxu124/OpenX-Embodiment",
                        "Text2CAD/Text2CAD",
                        "filapro/cad-recode"
                    ]
                },
                {
                    "name": "Stage 6: Code - Git & Software Engineering",
                    "order": 6,
                    "epochs": 1,
                    "description": "Understanding software lifecycles: commits, issues, PRs, and diffs.",
                    "datasets": [
                        "bigcode/commitpack",
                        "bigcode/commitpackft",
                        "HuggingFaceH4/self-oss-instruct-sc2-exec-filter-50k",
                        "google/code_contests",
                        "princeton-nlp/SWE-bench",
                        "bigcode/bigcodebench"
                    ]
                },
                {
                    "name": "Stage 7: Domain - Math, Science & Medicine",
                    "order": 7,
                    "epochs": 3,
                    "description": "Rigorous STEM: 20M+ Math problems, Quantum Physics, Chemistry, Biology, and Healthcare.",
                    "datasets": [
                        "lighteval/MATH",
                        "openai/gsm8k",
                        "meta-math/MetaMathQA",
                        "nvidia/OpenMathInstruct-2",
                        "AI-MO/NuminaMath-CoT",
                        "AI-MO/NuminaMath-TIR",
                        "VDR_Quantum",
                        "QuantumLLMInstruct",
                        "camel-ai/physics",
                        "camel-ai/chemistry",
                        "camel-ai/biology",
                        "jablonkagroup/ChemBench",
                        "openlifescienceai/medmcqa",
                        "medalpaca/medical_meadow_medqa",
                        "lavita/ChatDoctor-HealthCareMagic-100k",
                        "allenai/sciq",
                        "bigbio/pubmed_qa",
                        "climate_fever",
                        "MegaScience/TextbookReasoning"
                    ]
                },
                {
                    "name": "Stage 8: Domain - Professional & Reasoning",
                    "order": 8,
                    "epochs": 2,
                    "description": "Law, Finance, Advanced Logic/Reasoning, and R1 Distillation traces.",
                    "datasets": [
                        "HFforLegal/case-law",
                        "nguha/legalbench:abercrombie",
                        "PatronusAI/financebench",
                        "takala/financial_phrasebank",
                        "deepseek-ai/DeepSeek-R1-Distill-Qwen",
                        "simplescaling/s1-data",
                        "bespokelabs/Bespoke-Stratos-17k",
                        "Open-Orca/OpenOrca",
                        "Open-Orca/SlimOrca",
                        "kaist-ai/CoT-Collection",
                        "cais/mmlu",
                        "TIGER-Lab/MMLU-Pro",
                        "allenai/ai2_arc",
                        "Rowan/hellaswag",
                        "banking77",
                        "hotpot_qa"
                    ]
                },
                {
                    "name": "Stage 9: Skill - Code Instruction",
                    "order": 9,
                    "epochs": 2,
                    "description": "Instruction following for coding. Includes Benchmarks for capability (HumanEval, MBPP).",
                    "datasets": [
                        "ise-uiuc/Magicoder-Evol-Instruct-110K",
                        "nickrosh/Evol-Instruct-Code-80k-v1",
                        "m-a-p/CodeFeedback-Filtered-Instruction",
                        "TokenBender/code_instructions_122k_alpaca_style",
                        "codefuse-ai/Evol-instruction-66k",
                        "rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol",
                        "code-rag-bench/humaneval",
                        "code-rag-bench/mbpp",
                        "code-rag-bench/ds1000",
                        "bigcode/humanevalpack",
                        "yale-nlp/spider",
                        "jupyter-agent/jupyter-agent-dataset"
                    ]
                },
                {
                    "name": "Stage 10: Skill - General Instruction & Creative",
                    "order": 10,
                    "epochs": 2,
                    "description": "Rich conversational chat, roleplay, and creative writing.",
                    "datasets": [
                        "HuggingFaceH4/ultrachat_200k",
                        "stingning/ultrachat",
                        "anon8231489123/ShareGPT_Vicuna_unfiltered",
                        "WizardLMTeam/WizardLM_evol_instruct_V2_196k",
                        "teknium/OpenHermes-2.5",
                        "HuggingFaceH4/no_robots",
                        "roneneldan/TinyStories",
                        "euclaise/writingprompts",
                        "Gryphe/ChatGPT-4o-Writing-Prompts",
                        "Magpie-Align/Magpie-Pro-300K-Filtered"
                    ]
                },
                {
                    "name": "Stage 11: Skill - Agentic & Tool Use",
                    "order": 11,
                    "epochs": 2,
                    "description": "Function calling, API usage, plan execution. Critical for Claude Code/Codex CLI.",
                    "datasets": [
                        "THUDM/AgentInstruct",
                        "microsoft/orca-agentinstruct-1M-v1",
                        "Salesforce/xlam-function-calling-60k",
                        "glaiveai/glaive-function-calling-v2",
                        "NousResearch/hermes-function-calling-v1",
                        "nvidia/Nemotron-Agentic-v1",
                        "open-thoughts/OpenThoughts-Agent-v1-SFT",
                        "Nexusflow/VT_MultiAPIs",
                        "arcee-ai/agent-data"
                    ]
                },
                {
                    "name": "Stage 12: Alignment - Safety & Refinement",
                    "order": 12,
                    "epochs": 1,
                    "description": "Safety guardrails, refusal training, and preference alignment (DPO/RLHF prep)",
                    "datasets": [
                        "PKU-Alignment/PKU-SafeRLHF",
                        "PKU-Alignment/BeaverTails",
                        "nvidia/HelpSteer2",
                        "allenai/wildjailbreak",
                        "Anthropic/hh-rlhf",
                        "HuggingFaceH4/ultrafeedback_binarized",
                        "argilla/dpo-mix-7k"
                    ]
                }
            ],
            "hf_datasets": [
                "HuggingFaceFW/fineweb",
                "bigcode/the-stack-v2",
                "lighteval/MATH",
                "gsm8k",
                "openai/gsm8k",
                "HPC-AI/DeepSeek-R1-Distill-Qwen",
                "Salesforce/xlam-function-calling-60k"
            ]
        },
        "250m-balanced": {
            "name": "250M Balanced Baseline",
            "description": "Expanded curriculum for ~250M parameter models with emphasis on chat, instructions, reasoning, and agentic capabilities. 60+ curated datasets across 6 stages.",
            "version": "2.0.0",
            "target_params": "250M",
            "stages": [
                {
                    "name": "Stage 1: Foundation",
                    "order": 1,
                    "epochs": 5,
                    "description": "Core language understanding from high-quality educational and encyclopedic sources",
                    "datasets": [
                        "HuggingFaceFW/fineweb-edu",
                        "wikipedia",
                        "HuggingFaceTB/cosmopedia",
                        "bookcorpus",
                        "allenai/c4",
                        "openwebtext",
                        "cerebras/SlimPajama-627B"
                    ]
                },
                {
                    "name": "Stage 2: Chat & Instructions",
                    "order": 2,
                    "epochs": 10,
                    "description": "Primary focus: conversational AI, multi-turn dialogue, and instruction-following",
                    "datasets": [
                        "databricks/databricks-dolly-15k",
                        "OpenAssistant/oasst1",
                        "OpenAssistant/oasst2",
                        "HuggingFaceH4/no_robots",
                        "tatsu-lab/alpaca",
                        "yahma/alpaca-cleaned",
                        "teknium/openhermes",
                        "teknium/OpenHermes-2.5",
                        "LDJnr/Capybara",
                        "LDJnr/Pure-Dove",
                        "timdettmers/openassistant-guanaco",
                        "HuggingFaceH4/ultrachat_200k",
                        "stingning/ultrachat",
                        "HuggingFaceH4/ultrafeedback_binarized",
                        "Anthropic/hh-rlhf",
                        "WizardLM/WizardLM_evol_instruct_V2_196k",
                        "cognitivecomputations/dolphin",
                        "allenai/tulu-v2-sft-mixture",
                        "HuggingFaceH4/smoltalk"
                    ]
                },
                {
                    "name": "Stage 3: Agentic & Tool Calling",
                    "order": 3,
                    "epochs": 6,
                    "description": "Function calling, tool use, and agentic capabilities",
                    "datasets": [
                        "Salesforce/xlam-function-calling-60k",
                        "glaiveai/glaive-function-calling-v2",
                        "THUDM/AgentInstruct",
                        "microsoft/orca-agentinstruct-1M-v1",
                        "NousResearch/hermes-function-calling-v1"
                    ]
                },
                {
                    "name": "Stage 4: Code",
                    "order": 4,
                    "epochs": 5,
                    "description": "Programming and code generation capabilities",
                    "datasets": [
                        "nampdn-ai/tiny-codes",
                        "sahil2801/CodeAlpaca-20k",
                        "iamtarun/python_code_instructions_18k_alpaca",
                        "flytech/python-codes-25k",
                        "nickrosh/Evol-Instruct-Code-80k-v1",
                        "m-a-p/CodeFeedback-Filtered-Instruction",
                        "theblackcat102/evol-codealpaca-v1"
                    ]
                },
                {
                    "name": "Stage 5: Mathematics",
                    "order": 5,
                    "epochs": 8,
                    "description": "Mathematical problem-solving and numerical reasoning",
                    "datasets": [
                        "gsm8k",
                        "openai/gsm8k",
                        "lighteval/MATH",
                        "meta-math/MetaMathQA",
                        "TIGER-Lab/MathInstruct",
                        "nvidia/OpenMathInstruct-1",
                        "HuggingFaceH4/orca-math-word-problems-200k",
                        "qwedsacf/grade-school-math-instructions"
                    ]
                },
                {
                    "name": "Stage 6: Reasoning & Chain-of-Thought",
                    "order": 6,
                    "epochs": 10,
                    "description": "Logical reasoning, CoT prompting, and problem decomposition for COCONUT-style reasoning",
                    "datasets": [
                        "Open-Orca/OpenOrca",
                        "Open-Orca/SlimOrca",
                        "kaist-ai/CoT-Collection",
                        "GAIR/LIMA",
                        "google/boolq",
                        "piqa",
                        "hellaswag",
                        "winogrande",
                        "openbookqa",
                        "allenai/social_i_qa",
                        "allenai/commonsense_qa",
                        "allenai/cosmos_qa",
                        "ARC-Easy",
                        "ARC-Challenge",
                        "cais/mmlu"
                    ]
                }
            ],
            "hf_datasets": [
                "HuggingFaceFW/fineweb-edu",
                "wikipedia",
                "HuggingFaceTB/cosmopedia",
                "bookcorpus",
                "allenai/c4",
                "openwebtext",
                "cerebras/SlimPajama-627B",
                "databricks/databricks-dolly-15k",
                "OpenAssistant/oasst1",
                "OpenAssistant/oasst2",
                "HuggingFaceH4/no_robots",
                "tatsu-lab/alpaca",
                "yahma/alpaca-cleaned",
                "teknium/openhermes",
                "teknium/OpenHermes-2.5",
                "LDJnr/Capybara",
                "LDJnr/Pure-Dove",
                "timdettmers/openassistant-guanaco",
                "HuggingFaceH4/ultrachat_200k",
                "stingning/ultrachat",
                "HuggingFaceH4/ultrafeedback_binarized",
                "Anthropic/hh-rlhf",
                "WizardLM/WizardLM_evol_instruct_V2_196k",
                "cognitivecomputations/dolphin",
                "allenai/tulu-v2-sft-mixture",
                "HuggingFaceH4/smoltalk",
                "Salesforce/xlam-function-calling-60k",
                "glaiveai/glaive-function-calling-v2",
                "THUDM/AgentInstruct",
                "microsoft/orca-agentinstruct-1M-v1",
                "NousResearch/hermes-function-calling-v1",
                "nampdn-ai/tiny-codes",
                "sahil2801/CodeAlpaca-20k",
                "iamtarun/python_code_instructions_18k_alpaca",
                "flytech/python-codes-25k",
                "nickrosh/Evol-Instruct-Code-80k-v1",
                "m-a-p/CodeFeedback-Filtered-Instruction",
                "theblackcat102/evol-codealpaca-v1",
                "gsm8k",
                "openai/gsm8k",
                "lighteval/MATH",
                "meta-math/MetaMathQA",
                "TIGER-Lab/MathInstruct",
                "nvidia/OpenMathInstruct-1",
                "HuggingFaceH4/orca-math-word-problems-200k",
                "qwedsacf/grade-school-math-instructions",
                "Open-Orca/OpenOrca",
                "Open-Orca/SlimOrca",
                "kaist-ai/CoT-Collection",
                "GAIR/LIMA",
                "google/boolq",
                "piqa",
                "hellaswag",
                "winogrande",
                "openbookqa",
                "allenai/social_i_qa",
                "allenai/commonsense_qa",
                "allenai/cosmos_qa",
                "ARC-Easy",
                "ARC-Challenge",
                "cais/mmlu"
            ]
        }
    }
}
