{
    "version": "2.0.0",
    "last_updated": "2025-12-26",
    "description": "Comprehensive curriculum presets for frontier-class LLM training",
    "presets": {
        "verso-baseline": {
            "name": "Verso Baseline",
            "description": "Comprehensive frontier-class baseline curriculum covering all domains with 180+ datasets across 8 training stages",
            "version": "2.0.0",
            "stages": [
                {
                    "name": "Stage 1: Foundation - General Pre-training",
                    "order": 1,
                    "epochs": 1,
                    "description": "Massive web-scale pre-training for foundational language understanding",
                    "datasets": [
                        "HuggingFaceFW/fineweb",
                        "HuggingFaceFW/fineweb-edu",
                        "HuggingFaceFW/fineweb-2",
                        "allenai/c4",
                        "EleutherAI/pile",
                        "togethercomputer/RedPajama-Data-V2",
                        "cerebras/SlimPajama-627B",
                        "tiiuae/falcon-refinedweb",
                        "bigscience/roots",
                        "openwebtext",
                        "wikipedia",
                        "bookcorpus",
                        "cc100",
                        "mc4",
                        "oscar-corpus/OSCAR-2301",
                        "allenai/dolma",
                        "mosaicml/dolma",
                        "togethercomputer/RedPajama-Data-1T",
                        "CohereForAI/aya_collection",
                        "HuggingFaceTB/cosmopedia"
                    ]
                },
                {
                    "name": "Stage 2: Domain - Conversational AI",
                    "order": 2,
                    "epochs": 2,
                    "description": "Multi-turn dialogue, chat, and conversational AI",
                    "datasets": [
                        "databricks/databricks-dolly-15k",
                        "OpenAssistant/oasst1",
                        "OpenAssistant/oasst2",
                        "stingning/ultrachat",
                        "HuggingFaceH4/ultrachat_200k",
                        "HuggingFaceH4/ultrafeedback_binarized",
                        "Anthropic/hh-rlhf",
                        "lmsys/lmsys-chat-1m",
                        "anon8231489123/ShareGPT_Vicuna_unfiltered",
                        "RyokoAI/ShareGPT52K",
                        "openchat/openchat_sharegpt4_dataset",
                        "THUDM/AgentInstruct",
                        "fnlp/moss-002-sft-data",
                        "microsoft/orca-agentinstruct-1M-v1",
                        "Intel/orca_dpo_pairs",
                        "WizardLM/WizardLM_evol_instruct_V2_196k",
                        "cognitivecomputations/dolphin",
                        "argilla/distilabel-capybara-dpo-7k-binarized",
                        "tatsu-lab/alpaca",
                        "yahma/alpaca-cleaned",
                        "garage-bAInd/Open-Platypus",
                        "teknium/openhermes",
                        "LDJnr/Capybara",
                        "LDJnr/Pure-Dove",
                        "LDJnr/Puffin"
                    ]
                },
                {
                    "name": "Stage 3: Domain - Code Generation",
                    "order": 3,
                    "epochs": 2,
                    "description": "Code generation, completion, and programming proficiency",
                    "datasets": [
                        "bigcode/starcoderdata",
                        "bigcode/the-stack",
                        "bigcode/the-stack-dedup",
                        "bigcode/the-stack-v2",
                        "bigcode/the-stack-v2-dedup",
                        "codeparrot/github-code",
                        "codeparrot/github-code-clean",
                        "deepmind/code_contests",
                        "THUDM/CodeGeeX2-data",
                        "sahil2801/CodeAlpaca-20k",
                        "TokenBender/code_instructions_120k_alpaca_style",
                        "nickrosh/Evol-Instruct-Code-80k-v1",
                        "m-a-p/CodeFeedback-Filtered-Instruction",
                        "ise-uiuc/Magicoder-Evol-Instruct-110K",
                        "ise-uiuc/Magicoder-OSS-Instruct-75K",
                        "Elriggs/openwebmath",
                        "nampdn-ai/tiny-codes",
                        "glaiveai/glaive-code-assistant",
                        "HuggingFaceH4/self-oss-instruct-sc2-exec-filter-50k",
                        "ajibawa-2023/Code-290k-ShareGPT",
                        "bigcode/bigcodebench",
                        "bigcode/humanevalpack",
                        "cassanof/CodeLlama-Python-200k",
                        "flytech/python-codes-25k",
                        "iamtarun/python_code_instructions_18k_alpaca"
                    ]
                },
                {
                    "name": "Stage 4: Domain - Mathematics & Science",
                    "order": 4,
                    "epochs": 3,
                    "description": "Mathematical reasoning, problem-solving, and scientific knowledge",
                    "datasets": [
                        "lighteval/MATH",
                        "gsm8k",
                        "openai/gsm8k",
                        "meta-math/MetaMathQA",
                        "TIGER-Lab/MathInstruct",
                        "camel-ai/math",
                        "camel-ai/physics",
                        "camel-ai/chemistry",
                        "camel-ai/biology",
                        "EleutherAI/hendrycks_math",
                        "nvidia/OpenMathInstruct-1",
                        "nvidia/OpenMathInstruct-2",
                        "AI-MO/NuminaMath-CoT",
                        "AI-MO/NuminaMath-TIR",
                        "BAAI/Infinity-Math",
                        "mlfoundations/big-math",
                        "qwedsacf/grade-school-math-instructions",
                        "reasoning-machines/gsm-hard",
                        "deepmind/aqua_rat",
                        "allenai/sciq",
                        "allenai/scienceqa",
                        "GAIR/LIMA",
                        "HuggingFaceH4/orca-math-word-problems-200k",
                        "theblackcat102/evol-codealpaca-v1"
                    ]
                },
                {
                    "name": "Stage 5: Domain - Reasoning & Logic",
                    "order": 5,
                    "epochs": 2,
                    "description": "Logical reasoning, chain-of-thought, and problem decomposition",
                    "datasets": [
                        "Open-Orca/OpenOrca",
                        "Open-Orca/SlimOrca",
                        "euclaise/TinyOrcaSlimOrca",
                        "kaist-ai/CoT-Collection",
                        "GAIR/LIMA",
                        "allenai/natural-instructions",
                        "allenai/natural-instructions-v2",
                        "bigbench",
                        "cais/mmlu",
                        "lukaemon/mmlu",
                        "Muennighoff/flan",
                        "spacemanidol/flan2022",
                        "conceptofmind/FLAN_2022",
                        "google/boolq",
                        "ARC-Easy",
                        "ARC-Challenge",
                        "piqa",
                        "hellaswag",
                        "winogrande",
                        "openbookqa",
                        "allenai/social_i_qa",
                        "allenai/commonsense_qa",
                        "allenai/cosmos_qa",
                        "llm-wizard/WizardLM-Uncensored-SuperCOT"
                    ]
                },
                {
                    "name": "Stage 6: Instruction Following & Alignment",
                    "order": 6,
                    "epochs": 2,
                    "description": "Instruction-following, preference learning, and safety alignment",
                    "datasets": [
                        "teknium/OpenHermes-2.5",
                        "teknium/openhermes",
                        "Locutusque/TinyLlama-instruct",
                        "ajibawa-2023/General-Stories-Collection",
                        "silk-road/alpaca-data-gpt4-chinese",
                        "allenai/tulu-v2-sft-mixture",
                        "argilla/OpenHermesPreferences",
                        "mlabonne/guanaco-llama2-1k",
                        "timdettmers/openassistant-guanaco",
                        "HuggingFaceH4/no_robots",
                        "nvidia/Nemotron-Post-Training-Dataset-v2",
                        "HuggingFaceH4/smoltalk",
                        "BAAI/Infinity-Instruct",
                        "PKU-Alignment/PKU-SafeRLHF",
                        "PKU-Alignment/BeaverTails",
                        "allenai/WildChat-1M",
                        "stanfordnlp/shp",
                        "jondurbin/airoboros-3.2",
                        "jondurbin/truthy-dpo-v0.1",
                        "lmsys/toxic-chat",
                        "argilla/distilabel-intel-orca-dpo-pairs",
                        "nvidia/HelpSteer2",
                        "nvidia/HelpSteer",
                        "Skywork/Skywork-Reward-Preference-80K-v0.1"
                    ]
                },
                {
                    "name": "Stage 7: Multilingual & Cross-lingual",
                    "order": 7,
                    "epochs": 1,
                    "description": "Multilingual understanding and cross-lingual transfer",
                    "datasets": [
                        "cis-lmu/glot500",
                        "facebook/flores",
                        "allenai/wmt22",
                        "allenai/nllb",
                        "Helsinki-NLP/opus-100",
                        "gsarti/flores_101",
                        "Muennighoff/xP3",
                        "Muennighoff/xP3all",
                        "Muennighoff/xP3mt",
                        "bigscience/xP3",
                        "CohereForAI/aya_dataset",
                        "CohereForAI/aya_collection_language_split",
                        "facebook/xnli",
                        "paws-x",
                        "wiki_lingua",
                        "amazon_reviews_multi"
                    ]
                },
                {
                    "name": "Stage 8: Domain-Specific & Specialized",
                    "order": 8,
                    "epochs": 1,
                    "description": "Domain expertise in medicine, law, finance, and specialized fields",
                    "datasets": [
                        "medalpaca/medical_meadow_mediqa",
                        "medalpaca/medical_meadow_medical_flashcards",
                        "medalpaca/medical_meadow_wikidoc",
                        "medalpaca/medical_meadow_wikidoc_patient_information",
                        "pubmed_qa",
                        "allenai/biomed_qa",
                        "NousResearch/nous-clinical-instruct",
                        "pile-of-law/pile-of-law",
                        "nguha/legalbench",
                        "jonathanli/law-stack-exchange",
                        "AdaptLLM/finance-tasks",
                        "FinGPT/fingpt-sentiment-train",
                        "TheFinAI/flare-ner",
                        "news_commentary",
                        "ccdv/pubmed-summarization",
                        "allenai/qasper"
                    ]
                }
            ],
            "hf_datasets": [
                "HuggingFaceFW/fineweb",
                "HuggingFaceFW/fineweb-edu",
                "allenai/c4",
                "EleutherAI/pile",
                "togethercomputer/RedPajama-Data-V2",
                "cerebras/SlimPajama-627B",
                "tiiuae/falcon-refinedweb",
                "bigscience/roots",
                "openwebtext",
                "wikipedia",
                "HuggingFaceTB/cosmopedia",
                "databricks/databricks-dolly-15k",
                "OpenAssistant/oasst1",
                "stingning/ultrachat",
                "HuggingFaceH4/ultrafeedback_binarized",
                "Anthropic/hh-rlhf",
                "lmsys/lmsys-chat-1m",
                "bigcode/starcoderdata",
                "bigcode/the-stack-dedup",
                "codeparrot/github-code",
                "deepmind/code_contests",
                "lighteval/MATH",
                "gsm8k",
                "meta-math/MetaMathQA",
                "nvidia/OpenMathInstruct-1",
                "Open-Orca/OpenOrca",
                "teknium/OpenHermes-2.5",
                "CohereForAI/aya_dataset"
            ]
        },
        "chat-conversational": {
            "name": "Chat / Conversational",
            "description": "Comprehensive conversational AI and dialogue training",
            "hf_datasets": [
                "databricks/databricks-dolly-15k",
                "OpenAssistant/oasst1",
                "OpenAssistant/oasst2",
                "stingning/ultrachat",
                "HuggingFaceH4/ultrachat_200k",
                "HuggingFaceH4/ultrafeedback_binarized",
                "Anthropic/hh-rlhf",
                "lmsys/lmsys-chat-1m",
                "anon8231489123/ShareGPT_Vicuna_unfiltered",
                "RyokoAI/ShareGPT52K",
                "openchat/openchat_sharegpt4_dataset",
                "THUDM/AgentInstruct",
                "fnlp/moss-002-sft-data",
                "microsoft/orca-agentinstruct-1M-v1",
                "Intel/orca_dpo_pairs",
                "WizardLM/WizardLM_evol_instruct_V2_196k",
                "cognitivecomputations/dolphin",
                "argilla/distilabel-capybara-dpo-7k-binarized",
                "tatsu-lab/alpaca",
                "yahma/alpaca-cleaned",
                "garage-bAInd/Open-Platypus",
                "teknium/openhermes",
                "LDJnr/Capybara",
                "LDJnr/Pure-Dove",
                "LDJnr/Puffin"
            ]
        },
        "code-programming": {
            "name": "Code / Programming",
            "description": "Comprehensive code generation and programming assistance",
            "hf_datasets": [
                "bigcode/starcoderdata",
                "bigcode/the-stack",
                "bigcode/the-stack-dedup",
                "bigcode/the-stack-v2",
                "bigcode/the-stack-v2-dedup",
                "codeparrot/github-code",
                "codeparrot/github-code-clean",
                "deepmind/code_contests",
                "THUDM/CodeGeeX2-data",
                "sahil2801/CodeAlpaca-20k",
                "TokenBender/code_instructions_120k_alpaca_style",
                "nickrosh/Evol-Instruct-Code-80k-v1",
                "m-a-p/CodeFeedback-Filtered-Instruction",
                "ise-uiuc/Magicoder-Evol-Instruct-110K",
                "ise-uiuc/Magicoder-OSS-Instruct-75K",
                "Elriggs/openwebmath",
                "nampdn-ai/tiny-codes",
                "glaiveai/glaive-code-assistant",
                "HuggingFaceH4/self-oss-instruct-sc2-exec-filter-50k",
                "ajibawa-2023/Code-290k-ShareGPT",
                "bigcode/bigcodebench",
                "bigcode/humanevalpack",
                "cassanof/CodeLlama-Python-200k",
                "flytech/python-codes-25k",
                "iamtarun/python_code_instructions_18k_alpaca"
            ]
        },
        "reasoning-math": {
            "name": "Reasoning / Math",
            "description": "Advanced mathematical reasoning and problem solving",
            "hf_datasets": [
                "lighteval/MATH",
                "gsm8k",
                "openai/gsm8k",
                "meta-math/MetaMathQA",
                "TIGER-Lab/MathInstruct",
                "camel-ai/math",
                "camel-ai/physics",
                "camel-ai/chemistry",
                "camel-ai/biology",
                "EleutherAI/hendrycks_math",
                "nvidia/OpenMathInstruct-1",
                "nvidia/OpenMathInstruct-2",
                "AI-MO/NuminaMath-CoT",
                "AI-MO/NuminaMath-TIR",
                "BAAI/Infinity-Math",
                "mlfoundations/big-math",
                "qwedsacf/grade-school-math-instructions",
                "reasoning-machines/gsm-hard",
                "deepmind/aqua_rat",
                "allenai/sciq",
                "allenai/scienceqa",
                "GAIR/LIMA",
                "HuggingFaceH4/orca-math-word-problems-200k",
                "theblackcat102/evol-codealpaca-v1"
            ]
        },
        "general-language": {
            "name": "General Language",
            "description": "Massive-scale general language pre-training",
            "hf_datasets": [
                "HuggingFaceFW/fineweb",
                "HuggingFaceFW/fineweb-edu",
                "HuggingFaceFW/fineweb-2",
                "allenai/c4",
                "EleutherAI/pile",
                "togethercomputer/RedPajama-Data-V2",
                "cerebras/SlimPajama-627B",
                "tiiuae/falcon-refinedweb",
                "bigscience/roots",
                "openwebtext",
                "wikipedia",
                "bookcorpus",
                "cc100",
                "mc4",
                "oscar-corpus/OSCAR-2301",
                "allenai/dolma",
                "mosaicml/dolma",
                "togethercomputer/RedPajama-Data-1T",
                "CohereForAI/aya_collection",
                "HuggingFaceTB/cosmopedia"
            ]
        },
        "instruction-following": {
            "name": "Instruction Following",
            "description": "Comprehensive instruction-following and alignment",
            "hf_datasets": [
                "teknium/OpenHermes-2.5",
                "teknium/openhermes",
                "Locutusque/TinyLlama-instruct",
                "ajibawa-2023/General-Stories-Collection",
                "silk-road/alpaca-data-gpt4-chinese",
                "allenai/tulu-v2-sft-mixture",
                "argilla/OpenHermesPreferences",
                "mlabonne/guanaco-llama2-1k",
                "timdettmers/openassistant-guanaco",
                "HuggingFaceH4/no_robots",
                "nvidia/Nemotron-Post-Training-Dataset-v2",
                "HuggingFaceH4/smoltalk",
                "BAAI/Infinity-Instruct",
                "PKU-Alignment/PKU-SafeRLHF",
                "PKU-Alignment/BeaverTails",
                "allenai/WildChat-1M",
                "stanfordnlp/shp",
                "jondurbin/airoboros-3.2",
                "jondurbin/truthy-dpo-v0.1",
                "lmsys/toxic-chat",
                "argilla/distilabel-intel-orca-dpo-pairs",
                "nvidia/HelpSteer2",
                "nvidia/HelpSteer",
                "Skywork/Skywork-Reward-Preference-80K-v0.1"
            ]
        },
        "reasoning-logic": {
            "name": "Reasoning / Logic",
            "description": "Logical reasoning, chain-of-thought, and problem decomposition",
            "hf_datasets": [
                "Open-Orca/OpenOrca",
                "Open-Orca/SlimOrca",
                "euclaise/TinyOrcaSlimOrca",
                "kaist-ai/CoT-Collection",
                "GAIR/LIMA",
                "allenai/natural-instructions",
                "allenai/natural-instructions-v2",
                "bigbench",
                "cais/mmlu",
                "lukaemon/mmlu",
                "Muennighoff/flan",
                "spacemanidol/flan2022",
                "conceptofmind/FLAN_2022",
                "google/boolq",
                "ARC-Easy",
                "ARC-Challenge",
                "piqa",
                "hellaswag",
                "winogrande",
                "openbookqa",
                "allenai/social_i_qa",
                "allenai/commonsense_qa",
                "allenai/cosmos_qa",
                "llm-wizard/WizardLM-Uncensored-SuperCOT"
            ]
        },
        "multilingual": {
            "name": "Multilingual",
            "description": "Multilingual understanding and cross-lingual transfer",
            "hf_datasets": [
                "cis-lmu/glot500",
                "facebook/flores",
                "allenai/wmt22",
                "allenai/nllb",
                "Helsinki-NLP/opus-100",
                "gsarti/flores_101",
                "Muennighoff/xP3",
                "Muennighoff/xP3all",
                "Muennighoff/xP3mt",
                "bigscience/xP3",
                "CohereForAI/aya_dataset",
                "CohereForAI/aya_collection_language_split",
                "facebook/xnli",
                "paws-x",
                "wiki_lingua",
                "amazon_reviews_multi"
            ]
        },
        "domain-specialized": {
            "name": "Domain Specialized",
            "description": "Domain expertise in medicine, law, finance, and specialized fields",
            "hf_datasets": [
                "medalpaca/medical_meadow_mediqa",
                "medalpaca/medical_meadow_medical_flashcards",
                "medalpaca/medical_meadow_wikidoc",
                "medalpaca/medical_meadow_wikidoc_patient_information",
                "pubmed_qa",
                "allenai/biomed_qa",
                "NousResearch/nous-clinical-instruct",
                "pile-of-law/pile-of-law",
                "nguha/legalbench",
                "jonathanli/law-stack-exchange",
                "AdaptLLM/finance-tasks",
                "FinGPT/fingpt-sentiment-train",
                "TheFinAI/flare-ner",
                "news_commentary",
                "ccdv/pubmed-summarization",
                "allenai/qasper"
            ]
        },
        "250m-balanced": {
            "name": "250M Balanced Baseline",
            "description": "Expanded curriculum for ~250M parameter models with emphasis on chat, instructions, reasoning, and agentic capabilities. 60+ curated datasets across 6 stages.",
            "version": "2.0.0",
            "target_params": "250M",
            "stages": [
                {
                    "name": "Stage 1: Foundation",
                    "order": 1,
                    "epochs": 5,
                    "description": "Core language understanding from high-quality educational and encyclopedic sources",
                    "datasets": [
                        "HuggingFaceFW/fineweb-edu",
                        "wikipedia",
                        "HuggingFaceTB/cosmopedia",
                        "bookcorpus",
                        "allenai/c4",
                        "openwebtext",
                        "cerebras/SlimPajama-627B"
                    ]
                },
                {
                    "name": "Stage 2: Chat & Instructions",
                    "order": 2,
                    "epochs": 10,
                    "description": "Primary focus: conversational AI, multi-turn dialogue, and instruction-following",
                    "datasets": [
                        "databricks/databricks-dolly-15k",
                        "OpenAssistant/oasst1",
                        "OpenAssistant/oasst2",
                        "HuggingFaceH4/no_robots",
                        "tatsu-lab/alpaca",
                        "yahma/alpaca-cleaned",
                        "teknium/openhermes",
                        "teknium/OpenHermes-2.5",
                        "LDJnr/Capybara",
                        "LDJnr/Pure-Dove",
                        "timdettmers/openassistant-guanaco",
                        "HuggingFaceH4/ultrachat_200k",
                        "stingning/ultrachat",
                        "HuggingFaceH4/ultrafeedback_binarized",
                        "Anthropic/hh-rlhf",
                        "WizardLM/WizardLM_evol_instruct_V2_196k",
                        "cognitivecomputations/dolphin",
                        "allenai/tulu-v2-sft-mixture",
                        "HuggingFaceH4/smoltalk"
                    ]
                },
                {
                    "name": "Stage 3: Agentic & Tool Calling",
                    "order": 3,
                    "epochs": 6,
                    "description": "Function calling, tool use, and agentic capabilities",
                    "datasets": [
                        "Salesforce/xlam-function-calling-60k",
                        "glaiveai/glaive-function-calling-v2",
                        "THUDM/AgentInstruct",
                        "microsoft/orca-agentinstruct-1M-v1",
                        "NousResearch/hermes-function-calling-v1"
                    ]
                },
                {
                    "name": "Stage 4: Code",
                    "order": 4,
                    "epochs": 5,
                    "description": "Programming and code generation capabilities",
                    "datasets": [
                        "nampdn-ai/tiny-codes",
                        "sahil2801/CodeAlpaca-20k",
                        "iamtarun/python_code_instructions_18k_alpaca",
                        "flytech/python-codes-25k",
                        "nickrosh/Evol-Instruct-Code-80k-v1",
                        "m-a-p/CodeFeedback-Filtered-Instruction",
                        "theblackcat102/evol-codealpaca-v1"
                    ]
                },
                {
                    "name": "Stage 5: Mathematics",
                    "order": 5,
                    "epochs": 8,
                    "description": "Mathematical problem-solving and numerical reasoning",
                    "datasets": [
                        "gsm8k",
                        "openai/gsm8k",
                        "lighteval/MATH",
                        "meta-math/MetaMathQA",
                        "TIGER-Lab/MathInstruct",
                        "nvidia/OpenMathInstruct-1",
                        "HuggingFaceH4/orca-math-word-problems-200k",
                        "qwedsacf/grade-school-math-instructions"
                    ]
                },
                {
                    "name": "Stage 6: Reasoning & Chain-of-Thought",
                    "order": 6,
                    "epochs": 10,
                    "description": "Logical reasoning, CoT prompting, and problem decomposition for COCONUT-style reasoning",
                    "datasets": [
                        "Open-Orca/OpenOrca",
                        "Open-Orca/SlimOrca",
                        "kaist-ai/CoT-Collection",
                        "GAIR/LIMA",
                        "google/boolq",
                        "piqa",
                        "hellaswag",
                        "winogrande",
                        "openbookqa",
                        "allenai/social_i_qa",
                        "allenai/commonsense_qa",
                        "allenai/cosmos_qa",
                        "ARC-Easy",
                        "ARC-Challenge",
                        "cais/mmlu"
                    ]
                }
            ],
            "hf_datasets": [
                "HuggingFaceFW/fineweb-edu",
                "wikipedia",
                "HuggingFaceTB/cosmopedia",
                "bookcorpus",
                "allenai/c4",
                "openwebtext",
                "cerebras/SlimPajama-627B",
                "databricks/databricks-dolly-15k",
                "OpenAssistant/oasst1",
                "OpenAssistant/oasst2",
                "HuggingFaceH4/no_robots",
                "tatsu-lab/alpaca",
                "yahma/alpaca-cleaned",
                "teknium/openhermes",
                "teknium/OpenHermes-2.5",
                "LDJnr/Capybara",
                "LDJnr/Pure-Dove",
                "timdettmers/openassistant-guanaco",
                "HuggingFaceH4/ultrachat_200k",
                "stingning/ultrachat",
                "HuggingFaceH4/ultrafeedback_binarized",
                "Anthropic/hh-rlhf",
                "WizardLM/WizardLM_evol_instruct_V2_196k",
                "cognitivecomputations/dolphin",
                "allenai/tulu-v2-sft-mixture",
                "HuggingFaceH4/smoltalk",
                "Salesforce/xlam-function-calling-60k",
                "glaiveai/glaive-function-calling-v2",
                "THUDM/AgentInstruct",
                "microsoft/orca-agentinstruct-1M-v1",
                "NousResearch/hermes-function-calling-v1",
                "nampdn-ai/tiny-codes",
                "sahil2801/CodeAlpaca-20k",
                "iamtarun/python_code_instructions_18k_alpaca",
                "flytech/python-codes-25k",
                "nickrosh/Evol-Instruct-Code-80k-v1",
                "m-a-p/CodeFeedback-Filtered-Instruction",
                "theblackcat102/evol-codealpaca-v1",
                "gsm8k",
                "openai/gsm8k",
                "lighteval/MATH",
                "meta-math/MetaMathQA",
                "TIGER-Lab/MathInstruct",
                "nvidia/OpenMathInstruct-1",
                "HuggingFaceH4/orca-math-word-problems-200k",
                "qwedsacf/grade-school-math-instructions",
                "Open-Orca/OpenOrca",
                "Open-Orca/SlimOrca",
                "kaist-ai/CoT-Collection",
                "GAIR/LIMA",
                "google/boolq",
                "piqa",
                "hellaswag",
                "winogrande",
                "openbookqa",
                "allenai/social_i_qa",
                "allenai/commonsense_qa",
                "allenai/cosmos_qa",
                "ARC-Easy",
                "ARC-Challenge",
                "cais/mmlu"
            ]
        }
    }
}
