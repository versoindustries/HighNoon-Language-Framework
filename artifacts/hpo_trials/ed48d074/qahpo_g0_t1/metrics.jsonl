{"step": 10, "loss": 11.314582824707031, "perplexity": 82008.88095466075, "gradient_norm": 488.1976318359375, "learning_rate": 1.5064245827093326e-05, "epoch": 0, "memory_mb": 1928.33203125, "peak_memory_mb": 1928.33203125}
{"step": 20, "loss": 11.313855171203613, "perplexity": 81949.22861087324, "gradient_norm": 494.20172119140625, "learning_rate": 1.47938106117673e-05, "epoch": 0, "memory_mb": 1979.7890625, "peak_memory_mb": 1979.7890625}
{"step": 30, "loss": 11.312981605529785, "perplexity": 81877.67183708325, "gradient_norm": 487.7691955566406, "learning_rate": 1.4914983636090375e-05, "epoch": 0, "memory_mb": 1932.265625, "peak_memory_mb": 1979.7890625}
{"step": 40, "loss": 11.312210083007812, "perplexity": 81814.5257316678, "gradient_norm": 487.2706298828125, "learning_rate": 1.4955116240168986e-05, "epoch": 0, "memory_mb": 2072.22265625, "peak_memory_mb": 2072.22265625}
{"step": 50, "loss": 11.311734199523926, "perplexity": 81775.60081272769, "gradient_norm": 487.3741455078125, "learning_rate": 1.4971755443051972e-05, "epoch": 0, "memory_mb": 2032.49609375, "peak_memory_mb": 2072.22265625}
{"step": 60, "loss": 11.311049461364746, "perplexity": 81719.62510490003, "gradient_norm": 480.270751953125, "learning_rate": 1.4975326476129886e-05, "epoch": 1, "memory_mb": 1934.67578125, "peak_memory_mb": 2072.22265625}
{"step": 70, "loss": 11.310553550720215, "perplexity": 81679.10951982868, "gradient_norm": 502.3193054199219, "learning_rate": 1.4978847531151085e-05, "epoch": 1, "memory_mb": 1975.203125, "peak_memory_mb": 2072.22265625}
{"step": 80, "loss": 11.310240745544434, "perplexity": 81653.56386723215, "gradient_norm": 505.6920166015625, "learning_rate": 1.4976621371088352e-05, "epoch": 1, "memory_mb": 1995.1171875, "peak_memory_mb": 2072.22265625}
{"step": 90, "loss": 11.309611320495605, "perplexity": 81602.18524000175, "gradient_norm": 505.43206787109375, "learning_rate": 1.4979749716251257e-05, "epoch": 1, "memory_mb": 1975.390625, "peak_memory_mb": 2072.22265625}
{"step": 100, "loss": 11.309288024902344, "perplexity": 81575.80787718546, "gradient_norm": 495.6304626464844, "learning_rate": 1.4978607008972686e-05, "epoch": 1, "memory_mb": 1916.20703125, "peak_memory_mb": 2072.22265625}
{"step": 110, "loss": 11.309033393859863, "perplexity": 81555.03878852408, "gradient_norm": 496.3412170410156, "learning_rate": 1.4980012058373811e-05, "epoch": 2, "memory_mb": 1975.90625, "peak_memory_mb": 2072.22265625}
{"step": 120, "loss": 11.308541297912598, "perplexity": 81514.91575745732, "gradient_norm": 507.00164794921875, "learning_rate": 1.4976497806434263e-05, "epoch": 2, "memory_mb": 1936.4453125, "peak_memory_mb": 2072.22265625}
{"step": 130, "loss": 11.308476448059082, "perplexity": 81509.62969851303, "gradient_norm": 492.27642822265625, "learning_rate": 1.4975455919683002e-05, "epoch": 2, "memory_mb": 1976.8671875, "peak_memory_mb": 2072.22265625}
{"step": 140, "loss": 11.308065414428711, "perplexity": 81476.13338403532, "gradient_norm": 496.2203063964844, "learning_rate": 1.4976981984475343e-05, "epoch": 2, "memory_mb": 1977.2421875, "peak_memory_mb": 2072.22265625}
{"step": 150, "loss": 11.307883262634277, "perplexity": 81461.29371171341, "gradient_norm": 482.0975036621094, "learning_rate": 1.4974658743492947e-05, "epoch": 2, "memory_mb": 1977.3671875, "peak_memory_mb": 2072.22265625}
{"step": 160, "loss": 11.307720184326172, "perplexity": 81448.01022491253, "gradient_norm": 502.3968200683594, "learning_rate": 1.4976677016558595e-05, "epoch": 3, "memory_mb": 1978.47265625, "peak_memory_mb": 2072.22265625}
{"step": 170, "loss": 11.307413101196289, "perplexity": 81423.00275489254, "gradient_norm": 477.21502685546875, "learning_rate": 1.4975437838935883e-05, "epoch": 3, "memory_mb": 1978.47265625, "peak_memory_mb": 2072.22265625}
{"step": 180, "loss": 11.307291030883789, "perplexity": 81413.06403012552, "gradient_norm": 486.5111389160156, "learning_rate": 1.4973125859756997e-05, "epoch": 3, "memory_mb": 1919.0390625, "peak_memory_mb": 2072.22265625}
{"step": 190, "loss": 11.307238578796387, "perplexity": 81408.79385696605, "gradient_norm": 468.49298095703125, "learning_rate": 1.4972315251834793e-05, "epoch": 3, "memory_mb": 1958.87109375, "peak_memory_mb": 2072.22265625}
{"step": 200, "loss": 11.306859016418457, "perplexity": 81377.90000502816, "gradient_norm": 484.46240234375, "learning_rate": 1.4975518880262079e-05, "epoch": 3, "memory_mb": 2058.3828125, "peak_memory_mb": 2072.22265625}
{"step": 210, "loss": 11.306853294372559, "perplexity": 81377.43435828145, "gradient_norm": 460.1203918457031, "learning_rate": 1.4970808986968866e-05, "epoch": 4, "memory_mb": 2059.15625, "peak_memory_mb": 2072.22265625}
{"step": 220, "loss": 11.306681632995605, "perplexity": 81363.46619477798, "gradient_norm": 493.0967102050781, "learning_rate": 1.4973427041457946e-05, "epoch": 4, "memory_mb": 2061.96875, "peak_memory_mb": 2072.22265625}
{"step": 230, "loss": 11.306513786315918, "perplexity": 81349.81075317165, "gradient_norm": 477.1564025878906, "learning_rate": 1.4973010593238257e-05, "epoch": 4, "memory_mb": 2061.96875, "peak_memory_mb": 2072.22265625}
{"step": 240, "loss": 11.306446075439453, "perplexity": 81344.30267266589, "gradient_norm": 469.3811950683594, "learning_rate": 1.4971996894241394e-05, "epoch": 4, "memory_mb": 2062.0625, "peak_memory_mb": 2072.22265625}
{"step": 250, "loss": 11.306061744689941, "perplexity": 81313.04556277017, "gradient_norm": 467.0417785644531, "learning_rate": 1.4967547750856978e-05, "epoch": 4, "memory_mb": 2022.55078125, "peak_memory_mb": 2072.22265625}
