# highnoon/models/layers/hyperdimensional_layer.py
# Copyright 2025 Verso Industries (Author: Michael B. Zimmerman)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Hyperdimensional Embedding Layer for HighNoon Framework.

Phase 48: Hyperdimensional Quantum Embeddings (HQE)

This module provides memory-efficient embedding layers using holographic
representations and character-level HD encoding.

Key Features:
    - Standard embedding for active vocabulary (fast lookup)
    - Character-level HD encoding for rare/OOV tokens (O(hd_dim) memory)
    - CTQW semantic spreading for diffusion
    - 10-50x memory reduction vs full embedding tables

Architecture:
    Token IDs → Dual Path Routing → CTQW Spread → Output
                      ↓
    Active tokens → Standard Embedding
    Rare tokens → Character-level HD → Project

References:
    - Kanerva (2009): Hyperdimensional Computing
    - Plate (2003): Holographic Reduced Representations
    - HighNoon Phase 48 specifications
"""

from __future__ import annotations

import logging
import math
import warnings
from typing import Any

import tensorflow as tf

from highnoon import config
from highnoon._native.ops.circular_conv_op import circular_convolution
from highnoon._native.ops.hyperdimensional_embedding import ctqw_spread

# Phase 4.2: Suppress false-positive complex casting warnings
# The FFT->real extraction is mathematically correct but triggers TF warnings
warnings.filterwarnings("ignore", message=".*casting.*complex.*float.*")

logger = logging.getLogger(__name__)


class StreamingHDPosition(tf.keras.layers.Layer):
    """O(D) streaming position encoding using cyclic permutation.

    From Kanerva (2009): Position vectors are generated by cyclically
    shifting a single random base vector. This encodes unbounded
    positions with O(D) parameters instead of O(L × D).

    Properties:
    - pos[n] = permute(pos[0], n) — single base vector, shifted
    - Similarity decays smoothly with distance
    - Works with holographic binding (circular convolution)
    - CPU-friendly: just array indexing, no trig functions
    """

    def __init__(self, hd_dim: int, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self.hd_dim = hd_dim

    def build(self, input_shape: tf.TensorShape) -> None:
        # Use RandomNormal for 1D vector (Orthogonal requires 2D+)
        # High-dimensional random vectors are quasi-orthogonal (Johnson-Lindenstrauss)
        self.base_position = self.add_weight(
            name="base_position",
            shape=(self.hd_dim,),
            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0 / self.hd_dim**0.5),
            trainable=True,
        )
        super().build(input_shape)

    def get_position_vector(self, position: int | tf.Tensor) -> tf.Tensor:
        """Generate position vector via cyclic permutation."""
        if isinstance(position, int):
            shift = position % self.hd_dim
            return tf.roll(self.base_position, shift=shift, axis=0)

        # Vectorized for batch/sequence
        return self.get_position_vectors_batch(position)

    def get_position_vectors_batch(self, positions: tf.Tensor) -> tf.Tensor:
        """Batch position vector generation."""
        # Vectorized cyclic permutation via gather with shifted indices
        flat_pos = tf.reshape(positions, [-1])
        indices = tf.range(self.hd_dim)[None, :] - tf.cast(flat_pos[:, None], tf.int32)
        indices = tf.math.floormod(indices, self.hd_dim)
        pos_vectors = tf.gather(self.base_position, indices)

        target_shape = tf.concat([tf.shape(positions), [self.hd_dim]], axis=0)
        return tf.reshape(pos_vectors, target_shape)


class HyperdimensionalEmbedding(tf.keras.layers.Layer):
    """Memory-efficient embedding using holographic bundling.

    NOTE: This layer stores base_vectors[vocab_size, hd_dim] which is
    O(vocab × hd_dim). For memory-efficient embeddings, use DualPathEmbedding
    which uses character-level HD for rare tokens.

    This layer is kept for backward compatibility and specialized use cases.
    """

    def __init__(
        self,
        vocab_size: int,
        model_dim: int,
        hd_dim: int = 4096,
        num_bundles: int = 4,
        use_ctqw: bool = True,
        ctqw_steps: int = 3,
        max_seq_len: int = 8192,
        initializer: str = "glorot_uniform",
        **kwargs: Any,
    ) -> None:
        """Initialize HyperdimensionalEmbedding layer."""
        super().__init__(**kwargs)

        self.vocab_size = vocab_size
        self.model_dim = model_dim
        self.hd_dim = hd_dim
        self.num_bundles = num_bundles
        self.use_ctqw = use_ctqw
        self.ctqw_steps = ctqw_steps
        self.max_seq_len = max_seq_len
        self._initializer = initializer

        if hd_dim % model_dim != 0:
            raise ValueError(f"hd_dim ({hd_dim}) must be divisible by model_dim ({model_dim})")

        if hd_dim & (hd_dim - 1) != 0:
            next_pow2 = 2 ** math.ceil(math.log2(hd_dim))
            if next_pow2 % model_dim == 0:
                self.hd_dim = next_pow2
                logger.warning("hd_dim %d not power of 2, rounding to %d", hd_dim, self.hd_dim)

        logger.info(
            "[HDE] Initializing: vocab=%d, model_dim=%d, hd_dim=%d, ctqw=%s",
            vocab_size,
            model_dim,
            self.hd_dim,
            use_ctqw,
        )

    def build(self, input_shape: tf.TensorShape) -> None:
        """Build layer weights."""
        initializer = tf.keras.initializers.get(self._initializer)

        self.base_vectors = self.add_weight(
            name="base_vectors",
            shape=[self.vocab_size, self.hd_dim],
            initializer=initializer,
            trainable=True,
        )

        if getattr(config, "USE_STREAMING_HD_POSITIONS", True):
            self.streaming_pos = StreamingHDPosition(hd_dim=self.hd_dim, name="streaming_pos")
            self.streaming_pos.build([None])  # Explicitly build to create base_position weight
            self.position_keys = None
        else:
            self.position_keys = self.add_weight(
                name="position_keys",
                shape=[self.max_seq_len, self.hd_dim],
                initializer=initializer,
                trainable=True,
            )
            self.streaming_pos = None

        self.projection = self.add_weight(
            name="projection",
            shape=[self.hd_dim, self.model_dim],
            initializer=initializer,
            trainable=True,
        )

        super().build(input_shape)

    def call(self, token_ids: tf.Tensor, training: bool | None = None) -> tf.Tensor:
        """Forward pass computing holographic embeddings."""
        if len(token_ids.shape) == 3:
            token_ids = tf.squeeze(token_ids, axis=-1)

        batch_size = tf.shape(token_ids)[0]
        seq_len = tf.shape(token_ids)[1]

        token_embeds = tf.gather(self.base_vectors, token_ids)

        if self.streaming_pos is not None:
            # Generate positions based on absolute indices
            positions = tf.range(seq_len)
            pos_keys = self.streaming_pos.get_position_vectors_batch(positions)
        else:
            pos_keys = self.position_keys[:seq_len]

        bound_embeds = self._holographic_bind(token_embeds, pos_keys)

        if self.use_ctqw and self.ctqw_steps > 0:
            flat = tf.reshape(bound_embeds, [-1, self.hd_dim])
            spread = ctqw_spread(flat, steps=self.ctqw_steps)
            bound_embeds = tf.reshape(spread, [batch_size, seq_len, self.hd_dim])

        output = tf.einsum("bsh,hm->bsm", bound_embeds, self.projection)
        return output

    def _holographic_bind(self, token_embeds: tf.Tensor, pos_keys: tf.Tensor) -> tf.Tensor:
        """Bind token embeddings with position keys via circular convolution.

        Phase 4.1: Uses complex128 for consistency with quantum precision.
        """
        pos_keys = tf.expand_dims(pos_keys, 0)
        # Phase 4.1: Use complex128 for improved numerical stability
        tok_complex = tf.cast(token_embeds, tf.complex128)
        pos_complex = tf.cast(pos_keys, tf.complex128)
        tok_fft = tf.signal.fft(tok_complex)
        pos_fft = tf.signal.fft(pos_complex)
        bound_fft = tok_fft * pos_fft
        bound = tf.signal.ifft(bound_fft)
        # Cast back to float32 for downstream compatibility
        return tf.cast(tf.math.real(bound), tf.float32)

    def compute_output_shape(self, input_shape: tf.TensorShape) -> tf.TensorShape:
        return tf.TensorShape([input_shape[0], input_shape[1], self.model_dim])

    def get_config(self) -> dict[str, Any]:
        config = super().get_config()
        config.update(
            {
                "vocab_size": self.vocab_size,
                "model_dim": self.model_dim,
                "hd_dim": self.hd_dim,
                "num_bundles": self.num_bundles,
                "use_ctqw": self.use_ctqw,
                "ctqw_steps": self.ctqw_steps,
                "max_seq_len": self.max_seq_len,
                "initializer": self._initializer,
            }
        )
        return config


class DualPathEmbedding(tf.keras.layers.Layer):
    """Dual-path embedding: Standard + Character-Level HD.

    Combines standard embedding for frequently-used tokens with
    memory-efficient character-level hyperdimensional encoding for rare tokens.

    Memory Comparison (128K vocab, 256 dim, 4096 hd_dim):
    - Standard Embedding: 128K × 256 = 32M params
    - Old HDE (per-token): 128K × 4096 = 524M params (WORSE!)
    - This implementation: 10K × 256 + char-HD = ~3.5M params ✓

    Active vocabulary tokens use standard embedding (fast lookup).
    Rare/OOV tokens use character-level HD composition (memory-efficient).
    """

    def __init__(
        self,
        vocab_size: int,
        model_dim: int,
        active_vocab_size: int = 10000,
        hd_dim: int = 4096,
        use_ctqw: bool = True,
        ctqw_steps: int = 3,
        max_seq_len: int = 8192,
        max_char_per_token: int = 64,
        **kwargs: Any,
    ) -> None:
        """Initialize DualPathEmbedding.

        Args:
            vocab_size: Total vocabulary size.
            model_dim: Output dimension.
            active_vocab_size: Size of active (standard) vocabulary.
            hd_dim: Hyperdimensional space dimension for char-level encoding.
            use_ctqw: Apply CTQW spreading for semantic smoothing.
            ctqw_steps: CTQW diffusion steps.
            max_seq_len: Maximum sequence length (for API compatibility).
            max_char_per_token: Max characters per token for HD encoding.
        """
        super().__init__(**kwargs)

        self.vocab_size = vocab_size
        self.model_dim = model_dim
        self.active_vocab_size = min(active_vocab_size, vocab_size)
        self.hd_dim = hd_dim
        self.use_ctqw = use_ctqw
        self.ctqw_steps = ctqw_steps
        self.max_seq_len = max_seq_len
        self.max_char_per_token = max_char_per_token

        # Standard embedding for active vocabulary - O(active_vocab × model_dim)
        self.standard_embed = tf.keras.layers.Embedding(
            input_dim=self.active_vocab_size,
            output_dim=model_dim,
            name="standard_embedding",
        )

        # HD projection for rare tokens - O(hd_dim × model_dim)
        self.hd_projection = tf.keras.layers.Dense(
            model_dim,
            use_bias=True,
            name="hd_projection",
        )

        # Phase 901: Lift layer for active tokens to HD space (for holographic binding)
        # Projects from model_dim → hd_dim to enable circular convolution binding
        self.active_lift = tf.keras.layers.Dense(
            hd_dim,
            use_bias=False,  # No bias for clean linear lift
            name="active_lift_to_hd",
        )

        logger.info(
            "[DualPathEmbedding] vocab=%d, dim=%d, active=%d, hd_dim=%d (char-level HD)",
            vocab_size,
            model_dim,
            self.active_vocab_size,
            hd_dim,
        )

    @property
    def input_dim(self) -> int:
        """Return vocabulary size for API compatibility with tf.keras.layers.Embedding."""
        return self.vocab_size

    @property
    def output_dim(self) -> int:
        """Return model dimension for API compatibility with tf.keras.layers.Embedding."""
        return self.model_dim

    def compute_output_spec(self, inputs):
        """Return output spec for Keras functional API without triggering build().

        This allows Keras to infer output shapes during model construction
        without calling call(), which would fail because StreamingHDPosition
        weights (base_position) don't exist until build() is called.

        Args:
            inputs: Input tensor spec or tensor.

        Returns:
            KerasTensor spec with shape [batch, seq_len, model_dim].
        """
        return tf.keras.KerasTensor(
            shape=(inputs.shape[0], inputs.shape[1], self.model_dim),
            dtype=tf.float32,
        )

    def build(self, input_shape: tf.TensorShape) -> None:
        """Build layer weights.

        Creates memory-efficient character-level HD basis vectors:
        - hd_char_basis: [256, hd_dim] - one HD vector per byte/character
        - hd_position_basis: [max_char, hd_dim] - position binding

        Total HD params: 256 × hd_dim + 64 × hd_dim = ~1.3M for hd_dim=4096
        """
        # Character-level HD basis: [256, hd_dim] for byte values
        self.hd_char_basis = self.add_weight(
            name="hd_char_basis",
            shape=(256, self.hd_dim),
            initializer=tf.keras.initializers.Orthogonal(),
            trainable=True,
        )

        # Position-binding vectors for compositionality: [max_char, hd_dim]
        self.hd_position_basis = self.add_weight(
            name="hd_position_basis",
            shape=(self.max_char_per_token, self.hd_dim),
            initializer=tf.keras.initializers.Orthogonal(),
            trainable=True,
        )

        # Streaming position encoding for sequence length
        if getattr(config, "USE_STREAMING_HD_POSITIONS", True):
            self.streaming_pos = StreamingHDPosition(hd_dim=self.hd_dim, name="streaming_pos")
            self.streaming_pos.build([None])  # Explicitly build to create base_position weight
        else:
            self.streaming_pos = None

        super().build(input_shape)

    def call(self, token_ids: tf.Tensor, training: bool | None = None) -> tf.Tensor:
        """Forward pass with holographic position binding in HD space.

        Phase 901: All tokens are processed through HD space and bound with
        position vectors via circular convolution (Kanerva-style holographic
        binding). This preserves queryability: given a hidden state, we can
        decode "what token was at position X?" via correlation.

        Active tokens (< active_vocab_size) use standard embedding lifted to HD.
        Rare tokens (>= active_vocab_size) use character-level HD encoding.
        Both paths merge in HD space for holographic position binding.
        CTQW spreading applied after projection for semantic smoothing.

        Memory Optimization: Rare tokens are processed sparsely to avoid OOM.
        """
        # Import circular convolution for holographic binding

        batch_size = tf.shape(token_ids)[0]
        seq_len = tf.shape(token_ids)[1]

        # Create masks for active vs rare tokens
        active_mask = token_ids < self.active_vocab_size
        rare_mask = ~active_mask

        # === ACTIVE PATH: Lift to HD space ===
        clamped_ids = tf.minimum(token_ids, self.active_vocab_size - 1)
        active_embeds = self.standard_embed(clamped_ids)  # [B, L, model_dim]
        active_hd = self.active_lift(active_embeds)  # [B, L, hd_dim]

        # === RARE PATH: Already in HD space ===
        has_rare_tokens = tf.reduce_any(rare_mask)

        def _get_rare_hd():
            """Get HD embeddings for rare tokens (not projected yet)."""
            flat_ids = tf.reshape(token_ids, [-1])  # [batch * seq]
            flat_mask = tf.reshape(rare_mask, [-1])  # [batch * seq]

            # Get indices of rare tokens (sparse)
            rare_indices = tf.where(flat_mask)  # [num_rare, 1]
            rare_indices = tf.squeeze(rare_indices, axis=-1)  # [num_rare]

            # Gather only the rare token IDs
            rare_token_ids = tf.gather(flat_ids, rare_indices)  # [num_rare]

            # Encode to HD space (no projection yet)
            rare_hd = self._encode_rare_tokens_hd_only(rare_token_ids)  # [num_rare, hd_dim]

            # Scatter back to full tensor
            flat_hd = tf.zeros([batch_size * seq_len, self.hd_dim], dtype=tf.float32)
            rare_indices_2d = tf.expand_dims(rare_indices, 1)  # [num_rare, 1]
            flat_hd = tf.tensor_scatter_nd_update(flat_hd, rare_indices_2d, rare_hd)
            return tf.reshape(flat_hd, [batch_size, seq_len, self.hd_dim])

        def _zeros_hd():
            """Fast path: no rare tokens, return zeros in HD space."""
            return tf.zeros([batch_size, seq_len, self.hd_dim], dtype=tf.float32)

        rare_hd = tf.cond(has_rare_tokens, _get_rare_hd, _zeros_hd)

        # === MERGE IN HD SPACE ===
        # Use mask to combine: active where active, rare where rare
        active_mask_f = tf.cast(active_mask, tf.float32)[:, :, None]  # [B, L, 1]
        tokens_hd = active_hd * active_mask_f + rare_hd * (1.0 - active_mask_f)

        # === HOLOGRAPHIC BINDING (circular convolution) ===
        # Phase 901: Bind tokens with positions via FFT-based circular convolution
        # This is the core of Kanerva-style holographic computing:
        # bound = token ⊛ position (circular convolution in HD space)
        positions = tf.range(seq_len)
        pos_vectors = self.streaming_pos.get_position_vectors_batch(positions)  # [L, hd_dim]

        # Phase 900.2: Use C++ in-place circular convolution (4× memory reduction)
        # Automatically falls back to TensorFlow if C++ not compiled
        bound_hd = circular_convolution(
            tokens_hd, pos_vectors, hd_dim=self.hd_dim
        )  # [B, L, hd_dim]

        # L2 normalize for stability (important for HD computing)
        # GRADIENT FIX: Add epsilon to prevent NaN when norm is near zero
        bound_hd = tf.nn.l2_normalize(bound_hd, axis=-1, epsilon=1e-8)

        # === PROJECT TO MODEL DIM ===
        output = self.hd_projection(bound_hd)  # [B, L, model_dim]

        # === CTQW SPREADING (unchanged, in model_dim) ===
        if self.use_ctqw and self.ctqw_steps > 0:
            flat = tf.reshape(output, [-1, self.model_dim])
            spread = ctqw_spread(flat, steps=self.ctqw_steps)
            output = tf.reshape(spread, [batch_size, seq_len, self.model_dim])

        return output

    def _encode_rare_tokens_flat(self, token_ids: tf.Tensor) -> tf.Tensor:
        """Encode flat (1D) rare token IDs using character-level HD composition.

        Memory-efficient sparse encoding: only processes the actual rare tokens,
        not the entire batch×seq tensor.

        Args:
            token_ids: 1D tensor of rare token IDs [num_rare]

        Returns:
            HD embeddings projected to model_dim [num_rare, model_dim]
        """
        token_ids_int = tf.cast(token_ids, tf.int32)
        num_chars = 8

        # Create pseudo-characters from token ID via bit extraction
        char_indices = []
        for i in range(num_chars):
            shifted = tf.bitwise.right_shift(token_ids_int, i * 5)
            char_idx = tf.math.floormod(shifted, 256)
            char_indices.append(char_idx)

        # Stack: [num_rare, num_chars]
        char_indices = tf.stack(char_indices, axis=-1)

        # Gather HD vectors: [num_rare, num_chars, hd_dim]
        char_vectors = tf.gather(self.hd_char_basis, char_indices)

        # Position-bind each character
        pos_vectors = self.hd_position_basis[:num_chars, :]  # [num_chars, hd_dim]
        pos_vectors = tf.reshape(pos_vectors, [1, num_chars, self.hd_dim])

        # Bind: element-wise product
        bound_vectors = char_vectors * pos_vectors

        # Bundle: sum across characters -> [num_rare, hd_dim]
        hd_embedding = tf.reduce_sum(bound_vectors, axis=1)

        # Normalize for stability
        hd_embedding = tf.nn.l2_normalize(hd_embedding, axis=-1, epsilon=1e-8)

        # Project to model dimension -> [num_rare, model_dim]
        output = self.hd_projection(hd_embedding)

        return output

    def _encode_rare_tokens_hd_only(self, token_ids: tf.Tensor) -> tf.Tensor:
        """Encode rare tokens to HD space WITHOUT projecting to model_dim.

        Phase 901: Used for holographic binding path where projection happens
        after circular convolution binding with position vectors.

        Args:
            token_ids: 1D tensor of rare token IDs [num_rare]

        Returns:
            HD embeddings in hd_dim space [num_rare, hd_dim] (NOT projected)
        """
        token_ids_int = tf.cast(token_ids, tf.int32)
        num_chars = 8

        # Create pseudo-characters from token ID via bit extraction
        char_indices = []
        for i in range(num_chars):
            shifted = tf.bitwise.right_shift(token_ids_int, i * 5)
            char_idx = tf.math.floormod(shifted, 256)
            char_indices.append(char_idx)

        char_indices = tf.stack(char_indices, axis=-1)  # [num_rare, 8]
        char_vectors = tf.gather(self.hd_char_basis, char_indices)  # [num_rare, 8, hd_dim]

        # Position-bind each character (within-token character positions)
        pos_vectors = self.hd_position_basis[:num_chars, :]
        pos_vectors = tf.reshape(pos_vectors, [1, num_chars, self.hd_dim])

        # Bind: element-wise product
        bound_vectors = char_vectors * pos_vectors

        # Bundle: sum across characters -> [num_rare, hd_dim]
        hd_embedding = tf.reduce_sum(bound_vectors, axis=1)

        # Normalize for stability
        hd_embedding = tf.nn.l2_normalize(hd_embedding, axis=-1, epsilon=1e-8)

        # NO projection here - return HD space embedding for holographic binding
        return hd_embedding

    def _encode_rare_tokens(self, token_ids: tf.Tensor) -> tf.Tensor:
        """Legacy method: Encode rare tokens using character-level HD composition.

        DEPRECATED: Use _encode_rare_tokens_flat for memory-efficient sparse processing.
        This method is kept for backward compatibility only.

        Creates unique HD encoding by hashing token ID into character basis.
        Memory-efficient: O(hd_dim) regardless of vocab size.
        """
        token_ids_int = tf.cast(token_ids, tf.int32)

        # Create pseudo-characters from token ID via bit extraction
        # Use TensorFlow operations (not Python operators) for tensor compatibility
        num_chars = 8
        char_indices = []
        for i in range(num_chars):
            # tf.bitwise.right_shift and tf.math.floormod work on tensors
            shifted = tf.bitwise.right_shift(token_ids_int, i * 5)
            char_idx = tf.math.floormod(shifted, 256)
            char_indices.append(char_idx)

        # Stack: [batch, seq, num_chars]
        char_indices = tf.stack(char_indices, axis=-1)

        # Gather HD vectors: [batch, seq, num_chars, hd_dim]
        char_vectors = tf.gather(self.hd_char_basis, char_indices)

        # Position-bind each character
        pos_vectors = self.hd_position_basis[:num_chars, :]
        pos_vectors = tf.reshape(pos_vectors, [1, 1, num_chars, self.hd_dim])

        # Bind: element-wise product
        bound_vectors = char_vectors * pos_vectors

        # Bundle: sum across characters
        hd_embedding = tf.reduce_sum(bound_vectors, axis=2)

        # Normalize for stability
        hd_embedding = tf.nn.l2_normalize(hd_embedding, axis=-1, epsilon=1e-8)

        # Project to model dimension
        output = self.hd_projection(hd_embedding)

        return output

    def get_config(self) -> dict[str, Any]:
        config = super().get_config()
        config.update(
            {
                "vocab_size": self.vocab_size,
                "model_dim": self.model_dim,
                "active_vocab_size": self.active_vocab_size,
                "hd_dim": self.hd_dim,
                "use_ctqw": self.use_ctqw,
                "ctqw_steps": self.ctqw_steps,
                "max_seq_len": self.max_seq_len,
                "max_char_per_token": self.max_char_per_token,
            }
        )
        return config


__all__ = ["HyperdimensionalEmbedding", "DualPathEmbedding"]
